# Hidden Test Design for BioFabric-rs Agent Task

## Deep Audit of Public Tests

### Overview

The public test suite consists of **395 test functions** across 4 test files:

| File | Tests | What it validates |
|------|-------|-------------------|
| `parity_tests.rs` | 267 | Layout parity (NOA/EDA/BIF byte-level match against Java goldens) + property-based cycle tests |
| `analysis_tests.rs` | 65 | Graph analysis (cycle detection, Jaccard, components, topo sort, degree, alignment scoring) |
| `image_export_tests.rs` | 9 | Image export dimensions (PNG/JPEG/TIFF) |
| `cli_tests.rs` | 54 | End-to-end CLI integration |

### Fairness Assessment: Are the tests "hardcoded" to a particular Rust solution?

**Overall verdict: The tests are FAIR for a re-implementation task, with a few caveats.**

#### What makes them fair

1. **Golden-file based**: The core parity tests compare against golden files generated by the **Java reference implementation**, not against any Rust-specific behavior. An agent implementing the same algorithms will produce the same output regardless of internal code structure.

2. **Format-agnostic**: Tests compare text output (NOA, EDA, BIF XML) not internal data structures. The agent can use any internal representation as long as the output format matches.

3. **Well-documented algorithm specs**: The README has a detailed "Deterministic behavior reference" section specifying the exact algorithm rules (BFS node ordering, edge layout comparator, color assignment, etc.). This gives the agent a clear specification.

4. **Incremental progress**: Each parity test generates 3 independent functions (NOA, EDA, BIF), so the agent can track progress incrementally — getting NOA right first (node ordering), then EDA (edge layout), then BIF (full XML export).

5. **Analysis tests use computed values**: Cycle detection returns booleans, Jaccard returns floats, components returns sizes — none of these depend on implementation internals.

6. **CLI tests check behavioral contracts**: They test for string patterns in output (e.g., "Nodes:", "Links:", "json"), not specific formatting.

#### Potential fairness issues (and mitigations)

1. **Test harness imports specific Rust types**: The `parity_tests.rs` file imports ~30 specific types from `biofabric_core` (e.g., `DefaultNodeLayout`, `DefaultEdgeLayout`, `LayoutBuildData`, `NodeSimilarityLayout`, etc.). The agent must implement these exact public APIs for the tests to compile.

   **Mitigation**: This is actually GOOD for the task — it defines the API contract the agent must implement. The skeleton should preserve all these type signatures as stubs.

2. **Internal field access in tests**: Tests directly access fields like `layout.nodes`, `layout.links`, `layout.column_count`, `nl.row`, `nl.min_col`, `ll.column`, `ll.is_shadow`, etc. These constrain the data structure layout.

   **Mitigation**: The skeleton should define these structs with the correct field names and types. The agent fills in the logic.

3. **Extract submodel logic in test harness**: The `extract_submodel()` function in `parity_tests.rs` (lines 1268-1530) contains 260+ lines of complex extraction/compression logic that replicates a Java bug. This is essentially implementation code living in the test harness.

   **Assessment**: This is fine — it's test infrastructure that the agent doesn't need to implement. It's in the test file, not the library.

4. **Format functions in test harness**: `format_noa()`, `format_eda()`, `format_eda_no_shadows()` are defined in the test harness (not the library). These define the exact output format.

   **Assessment**: This is fine for the task. The agent implements the library; the test harness handles formatting.

5. **Set layout annotation logic**: Lines 516-689 of `parity_tests.rs` contain complex set layout annotation building logic. This is test infrastructure code that's essentially part of the "correct answer."

   **Assessment**: Acceptable — this is test harness setup, not the core algorithm. The agent needs to implement `SetLayout` and `DefaultEdgeLayout`, and the test harness wires them together.

6. **Alignment-related tests reference `.scores` golden files**: These tests read golden `output.scores` files and compare computed scores with 1e-6 tolerance.

   **Assessment**: Fair — the scores are algorithm outputs, not implementation details.

### Skeleton Requirements

For the agent task, the skeleton must preserve:

1. **All public type signatures** imported by the test files
2. **All public field names** on structs accessed by tests
3. **All public method signatures** called by tests
4. **File structure** (module paths like `biofabric_core::layout::default::DefaultNodeLayout`)
5. **Cargo.toml dependencies** and feature flags
6. **Test files themselves** (unmodified)
7. **Network input files** (`tests/parity/networks/`)
8. **Golden generation infrastructure** (Dockerfile, generate_goldens.sh, GoldenGenerator.java)

---

## Hidden Test Design

### Strategy

Create 2-3 hidden tests that exercise the same code paths as the public tests but with different input networks/alignments. This prevents an agent from "overfitting" to the specific golden files.

### What the hidden tests should cover

Based on the public test phases, the hidden tests should cover:

1. **Default layout parity (P1/P2)** — A new SIF network with shadows ON and OFF
2. **Alignment pipeline (P15-P17)** — A new pair of GW networks with a new alignment file
3. **Analysis operations (P13)** — Cycle detection, Jaccard, components on the new network

### Input data you need to procure

#### Hidden Test 1: New SIF Network (Layout Parity)

Create a new SIF file with ~10-20 nodes that exercises:
- Multiple relation types (like `multi_relation.sif`)
- At least one self-loop
- At least one disconnected component
- A mix of high-degree and low-degree nodes

Example structure:
```
W pp X
X pp Y
Y pp Z
W pp Z
X pd V
V pd U
U gi W
T pp T
Q pp R
```

Name it something like `hidden_mixed.sif`.

**Tests to run:**
- Default layout, shadows ON → compare NOA, EDA, BIF against golden
- Default layout, shadows OFF → compare NOA, EDA, BIF against golden
- Connected components count
- Jaccard similarity for a specific pair
- Cycle detection

#### Hidden Test 2: New GW Network Pair + Alignment

Create two small GW networks (~50-100 nodes each) and an alignment file. These can be synthetic (random graphs with known alignment).

**Tests to run:**
- Alignment GROUP view → compare NOA, EDA, BIF, scores against golden
- Alignment scoring (EC, S3, ICS, NC if perfect alignment provided)

#### Hidden Test 3: New DAG + Hierarchy

Create a small DAG in SIF format (~8-12 nodes) to test HierDAG layout.

**Tests to run:**
- HierDAG layout (pointUp=true) → compare NOA, EDA, BIF
- Topological sort level assignments

### How to generate golden files

Since golden files are ~800MB and can't go in git, they must be generated in the Dockerfile:

```dockerfile
# In the task Dockerfile, after cloning the repo:
# 1. Build the Java golden generator (existing Dockerfile logic)
# 2. Copy hidden network files into the container
# 3. Run GoldenGenerator for each hidden test case
# 4. Store golden files at a known path inside the container
```

The hidden test runner would:
1. Check for golden files at a predefined path (e.g., `/goldens/hidden/`)
2. If not present, generate them using the Rust `generate_goldens` function (since Rust output = Java output)
3. Compare the agent's output against these goldens

### Hidden test file structure

```
tests/parity/
├── networks/
│   ├── sif/
│   │   └── hidden_mixed.sif          # NEW - hidden network
│   ├── gw/
│   │   ├── hidden_net1.gw            # NEW - hidden alignment network 1
│   │   └── hidden_net2.gw            # NEW - hidden alignment network 2
│   ├── align/
│   │   └── hidden_align.align        # NEW - hidden alignment mapping
│   └── sif/
│       └── hidden_dag.sif            # NEW - hidden DAG
├── goldens/
│   └── hidden/                       # Generated at Docker build time
│       ├── hidden_mixed_default/
│       ├── hidden_mixed_noshadow/
│       ├── hidden_dag_hierdag_true/
│       ├── hidden_align_group/
│       └── ...
└── hidden_manifest.toml              # Hidden test manifest
```

### Recommended hidden test manifest (TOML)

```toml
# Hidden test cases — generated at Docker build time, not in git

[[test]]
id = "hidden_mixed_default"
description = "Hidden mixed network, default layout, shadows ON"
input = "hidden_mixed.sif"
golden_dir = "hidden_mixed_default"
layout = "default"
shadows = true
compare = ["noa", "eda", "bif"]

[[test]]
id = "hidden_mixed_noshadow"
description = "Hidden mixed network, default layout, shadows OFF"
input = "hidden_mixed.sif"
golden_dir = "hidden_mixed_noshadow"
layout = "default"
shadows = false
compare = ["noa", "eda", "bif"]

[[test]]
id = "hidden_dag_hierdag_true"
description = "Hidden DAG, HierDAG layout, pointUp=true"
input = "hidden_dag.sif"
golden_dir = "hidden_dag_hierdag_true"
layout = "hierdag"
shadows = true
point_up = true
compare = ["noa", "eda", "bif"]

[[test]]
id = "hidden_align_group"
description = "Hidden alignment, GROUP view"
input = "hidden_net1.gw"
golden_dir = "hidden_align_group"
layout = "alignment"
shadows = true
align_net2 = "hidden_net2.gw"
align_file = "hidden_align.align"
compare = ["noa", "eda", "bif", "scores"]
```

### Dockerfile additions for hidden golden generation

Add to the existing Dockerfile:

```dockerfile
# --- Hidden test golden generation ---
# Copy hidden network files
COPY tests/parity/networks/sif/hidden_*.sif /hidden_networks/sif/
COPY tests/parity/networks/gw/hidden_*.gw /hidden_networks/gw/
COPY tests/parity/networks/align/hidden_*.align /hidden_networks/align/

# Generate hidden goldens
RUN mkdir -p /goldens/hidden && \
    java -Djava.awt.headless=true -Xmx4g -cp /build/classes GoldenGenerator \
        /hidden_networks/sif/hidden_mixed.sif /goldens/hidden/hidden_mixed_default && \
    java -Djava.awt.headless=true -Xmx4g -cp /build/classes GoldenGenerator \
        /hidden_networks/sif/hidden_mixed.sif /goldens/hidden/hidden_mixed_noshadow --no-shadows && \
    java -Djava.awt.headless=true -Xmx4g -cp /build/classes GoldenGenerator \
        /hidden_networks/sif/hidden_dag.sif /goldens/hidden/hidden_dag_hierdag_true --hierdag --point-up && \
    echo "Hidden goldens generated."
```

### Hidden test runner (Rust)

Create `crates/core/tests/hidden_tests.rs` that mirrors `parity_tests.rs` but:
- Uses the hidden network files
- Reads goldens from `/goldens/hidden/` or a configured path
- Only runs the 2-3 hidden test cases

This file would be included in the task's grading harness but NOT given to the agent.

### Summary: What you need to create

| Item | Who creates | Where it lives |
|------|-------------|---------------|
| Hidden network files (2-3 .sif/.gw/.align files) | You | `tests/parity/networks/` (separate from public, or in a hidden location) |
| Hidden test manifest | You | `tests/parity/hidden_manifest.toml` (not given to agent) |
| Hidden Rust test file | You | `crates/core/tests/hidden_tests.rs` (not given to agent) |
| Hidden golden generation in Dockerfile | You | Extend existing Dockerfile |
| Hidden golden files | Dockerfile builds them | Generated at container build time |

### Key insight for Harbor format (no Docker-in-Docker)

Since Harbor can't do Docker-in-Docker, generate the golden files **during the Docker image build** (`docker build` phase, not `docker run`). The Dockerfile `RUN` commands execute the Java golden generator, and the golden files become part of the container image. At runtime, the hidden test runner simply reads them from the filesystem.

Alternatively, since your Rust implementation produces identical output to Java, you could:
1. Pre-generate the hidden goldens using your working Rust implementation
2. Embed them in the Docker image as static files
3. The hidden test runner reads them at test time

This avoids needing Java at all during the agent's runtime.
